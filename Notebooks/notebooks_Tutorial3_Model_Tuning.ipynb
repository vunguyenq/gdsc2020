{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Model Tuning\n",
    "\n",
    "In tutorial 2, we build a baseline model based on image embeddings and the cosine similarity and trained a distance metric between images with a deep learning model. To our surprise, the deep learning model performed worse than the cosine model. \n",
    "\n",
    "In this tutorial we will\n",
    "- Analyse and fix the problems in our current model\n",
    "- Learn how to use AWS to train larger and better models\n",
    "- Discuss potential further improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Why is the Deep Learning Model worse than the baseline?](#section_explain_results) <br>\n",
    "[2. Creating better training pairs](#section_better_training_data) <br>\n",
    "[3. Retraining the model](#section_retraining) <br>\n",
    "[4. Learning embeddings](#section_learning_embeddings) <br>\n",
    "[5. Intro to AWS](#aws) <br>\n",
    "[6. Setting up AWS SageMaker](#aws_setup) <br>\n",
    "&emsp; [6.1 AWS Account Credentials](#aws_setup.credentials) <br>\n",
    "&emsp; [6.2 AWS Account Setup](#aws_setup.account_setup) <br>\n",
    "&emsp; [6.3 The Training Script](#aws_setup.training_script) <br>\n",
    "&emsp; [6.4 Training in the Cloud](#aws_setup.training_in_the_cloud) <br>\n",
    "&emsp; [6.5 Getting the results](#aws_setup.getting_the_results) <br>\n",
    "&emsp; [6.6 Getting the results](#aws_setup.getting_the_results) <br>\n",
    "&emsp; [6.7 Rules](#aws_setup.rules) <br>\n",
    "[7. Next steps](#next_steps) <br>\n",
    "[8. Summary](#summary) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project dir and the src folder to paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_dir = Path.cwd().parent\n",
    "src_dir = project_dir / 'src'\n",
    "sys.path.insert(0, str(project_dir))\n",
    "sys.path.insert(0, str(src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability, we list all libraries we use in this notebook at the beginning\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from itertools import combinations, permutations\n",
    "from data.embedding_generators import DataGeneratorFromEmbeddings, BalancedDataGeneratorFromEmbeddings\n",
    "from data.image_generators import load_image\n",
    "from utils.remote_sagemaker import get_job_details\n",
    "from utils.remote_sagemaker import get_job_details\n",
    "from utils.remote_sagemaker import start_remote_sagemaker_job\n",
    "from utils.remote_sagemaker import upload_code_folder_to_s3\n",
    "from models.siamese_twin_embeddings import siamese_net_from_embeddings\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_explain_results'></a>\n",
    "# Why is the Deep Learning Model worse than the baseline?\n",
    "\n",
    "Let's recap our approach. We\n",
    "- Computed embeddings of the images\n",
    "- Defined the architecture of the model\n",
    "- Generated image pairs as training data\n",
    "- Fit the model to our data\n",
    "- Created predictions\n",
    "\n",
    "There must be a mistake in at least one of these steps. \n",
    "\n",
    "Since we used the same embeddings for the baseline model. The first step is probably not the issue. \n",
    "The architecture is very simple. Hence also an unlikely candidate.\n",
    "The accuracy ($>98.5\\%$) reported by the *fit* function indicates that our model did get pretty good at matching the training data. \n",
    "\n",
    "Such a high accuracy is usually an indicator that we are overfitting, i.e. that the model performs very well on the training data, but badly on new data. For deep learning models, this typically appears when your train for very long and the solution would be to stop the training earlier. \n",
    "\n",
    "In our case, a high accuracy was reached almost immediately. Our model found it *very simple* to solve the task which seems weird given the use case. Let's examine that data that we used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = Path('../data/train')\n",
    "train_files_paths = list(train_data_path.glob('*/*.jpg'))\n",
    "train_files_names = [p.name for p in train_files_paths]\n",
    "train_files_classes = [p.parent.name for p in train_files_paths]\n",
    "train_pairs = list(combinations(zip(train_files_names, train_files_classes), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(pair):\n",
    "    image_one_class = pair[0][1]\n",
    "    image_two_class = pair[1][1]\n",
    "    if image_one_class == image_two_class:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = {pair: is_similar(pair) for pair in train_pairs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of all possible image combinations. A pair gets the label *1* iff both pictures show the same whale. If the images show a different whale the label is *0*. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of pairs is 10267246\n",
      "10094602 pairs have the label 0, i.e. show different whales.\n",
      "172644 pairs have the label 1, i.e. show the same whale.\n"
     ]
    }
   ],
   "source": [
    "total = len(train_labels)\n",
    "neg_count = sum(value == 0 for value in train_labels.values())\n",
    "pos_count = sum(value == 1 for value in train_labels.values())\n",
    "print(f\"The total number of pairs is {total}\")\n",
    "print(f\"{neg_count} pairs have the label 0, i.e. show different whales.\")\n",
    "print(f\"{pos_count} pairs have the label 1, i.e. show the same whale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely imbalanced. Out of over 10 million examples only around 170.000 have the label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100.0*pos_count/total, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e. only $1,68\\%$ of the pairs show the same whale. Given that we only used $32*100*10 = 32.000$ pairs (batch size * steps * epochs) for training, the chance that our model learned always output *0* is pretty high.\n",
    "\n",
    "But this is not all that went wrong. Recall that we learned in the EDA that the class with the most images was the *-1* class, i.e. the class for images that were too bad for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 165600 pairs of bad pictures.\n"
     ]
    }
   ],
   "source": [
    "bad_pics = set(train_data_path.glob('-1/*.jpg'))\n",
    "bad_pairs = len(list(combinations(zip(bad_pics, bad_pics), 2)))\n",
    "print(f\"There are {bad_pairs} pairs of bad pictures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100.0*(pos_count - bad_pairs)/total, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$0.07\\%$ of all image pairs that show the same whale. Therefore, we can explain the results of our deep learing model with *the number one rule of machine learning*:\n",
    "\n",
    "**Garbage in - Garbage out: If training data is bad, the model predictions are useless**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- Always remember the number one rule of machine learning: **Garbage in - Garbage out**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_better_training_data'></a>\n",
    "# Creating better training pairs\n",
    "\n",
    "Let's create better data to train from. We will\n",
    "\n",
    "- Ignore all image from the -1 class\n",
    "- Ignore all whale ids with only one image\n",
    "- Create a dataset where both labels appear 50% of the time\n",
    "\n",
    "This requires some changes to our previous DataGenerator: *DataGeneratorFromEmbeddings*. The result is called *BalancedDataGeneratorFromEmbeddings* and can also be found in `src/data/embedding_generators`.\n",
    "\n",
    "To start, we need to define our model and compute the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "mobilenet = MobileNet(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = tf.data.Dataset.from_tensor_slices(list(map(str, train_files_paths)))  \n",
    "train_ds = train_files.map(load_image)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "embed_train = mobilenet.predict(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = {img: embedding for img, embedding in zip(train_files_names, embed_train)}\n",
    "train_generator = BalancedDataGeneratorFromEmbeddings(Path('../data/train'), train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *BalancedDataGeneratorFromEmbeddings* takes the path to the training data and the precomputed train embeddings and creates balanced samples. When you check out the code, you'll find that the *__parse_folder* function removes the -1 class as well as all whale ids with only one image:\n",
    "\n",
    "```python\n",
    "    def __parse_folder(self):\n",
    "        \"\"\"\n",
    "        Creates id_to_images and image_to_ids mappings.\n",
    "        Ignores all whale ids with only one image and the -1 class\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        id_folders = list(self.train_data_path.glob('*'))\n",
    "        id_to_images = {}\n",
    "        image_to_id = {}\n",
    "        for folder in id_folders:\n",
    "            if folder.name == \"-1\":\n",
    "                continue\n",
    "            pics = list(folder.glob('*.jpg'))\n",
    "            if len(pics) == 1:\n",
    "                continue\n",
    "            id_to_images[folder.name] = {p.name for p in pics}\n",
    "            for p in pics:\n",
    "                image_to_id[p.name] = folder.name\n",
    "        return id_to_images, image_to_id\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balancing is done in the *__data_generation* function:\n",
    "\n",
    "```python\n",
    "    def __data_generation(self, images):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples\n",
    "        :param image_to_id_temp:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        x = [np.empty((self.batch_size, self.dim)) for _i in range(2)]\n",
    "        y = np.empty(self.batch_size, dtype=int)\n",
    "        # Generate data\n",
    "        for i, image in enumerate(images):\n",
    "            output_index = 2*i\n",
    "\n",
    "            # Store similar sample\n",
    "            similar_img = self.__get_similar_image(image)\n",
    "            x[0][output_index, ] = self.embeddings[image]\n",
    "            x[1][output_index, ] = self.embeddings[similar_img]\n",
    "            y[output_index] = 1\n",
    "\n",
    "            # Store different sample\n",
    "            different_img = self.__get_different_image(image)\n",
    "            x[0][output_index+1, ] = self.embeddings[image]\n",
    "            x[1][output_index+1, ] = self.embeddings[different_img]\n",
    "            y[output_index+1] = 0\n",
    "\n",
    "        return x, y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the balancing by taking a sample and checking the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_generator.get_sample(0)\n",
    "sample[1]  # Show classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we fixed all issues we identified. Let's retrain the model and check the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_retraining'></a>\n",
    "# Retraining the model\n",
    "\n",
    "We reuse the model from tutorial 2. For readability, we put the definition code in a separate file: `src/models/siamese_twin_embeddings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(None, 1024)]       0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs (TensorFlowOpLa [(None, 1024)]       0           tf_op_layer_sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_Abs[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,025\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = siamese_net_from_embeddings(0.0001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.7012 - accuracy: 0.5521\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6632 - accuracy: 0.6005\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.6401 - accuracy: 0.62820s - loss:\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 0.6223 - accuracy: 0.6558\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.6077 - accuracy: 0.6737\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.5962 - accuracy: 0.6858\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.5863 - accuracy: 0.7010\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5790 - accuracy: 0.7053\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5687 - accuracy: 0.7164\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5649 - accuracy: 0.7129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3f802080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=500, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference in accuracy and loss. In tutorial 2, we got an accuracy of ~98% almost immediately.\n",
    "This time, the accuracy is much lower which seems more realistic. But what kind of accuracy would we need?\n",
    "\n",
    "Recall that for each test image, we have to predict the similariy for over 5000 images. \n",
    "With an accuracy of around 70%, the chance that the correct images are within the top 20 predictions are very low.\n",
    "\n",
    "**Excercise:**\n",
    "- Create and submit new predictions with the newly trained model. What kind of score do you get?\n",
    "- How does the accuracy and score change when you train the model for longer?\n",
    "- How does the accuracy and score change when you add additional layers to the model?\n",
    "- Check the *get_similar_image()* function of the BalancedDataGeneratorFromEmbeddings class. Is this the best way to find similar images? What happens if you change it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_learning_embeddings'></a>\n",
    "# Learning embeddings\n",
    "\n",
    "When the accuracy of your model isn't good it is either due to bad training data, because we didn't train long enough, or because the model isn't powerful enough.\n",
    "In this section, we'll move away from precomputed embeddings and train all of MobileNet instead. The resulting model will have many more parameters and should achieve a higher accuracy. We need to make several changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_from_scratch.generator'></a>\n",
    "### Updating the DataGenerator\n",
    "\n",
    "The changes we have to make to our DataGenerator class are rather small: Instead of loading the embeddings, we are now loading the pictures directly. \n",
    "\n",
    "In *BalancedDataGeneratorFromEmbeddings* we created the input data via:\n",
    "```python\n",
    "            output_index = 2 * i\n",
    "    \n",
    "            # Store similar sample\n",
    "            similar_img = self.__get_similar_image(image)\n",
    "            x[0][output_index, ] = self.embeddings[image]\n",
    "            x[1][output_index, ] = self.embeddings[similar_img]\n",
    "            y[output_index] = 1\n",
    "\n",
    "            # Store different sample\n",
    "            different_img = self.__get_different_image(image)\n",
    "            x[0][output_index+1, ] = self.embeddings[image]\n",
    "            x[1][output_index+1, ] = self.embeddings[different_img]\n",
    "            y[output_index+1] = 0\n",
    "```\n",
    "\n",
    "This needs to be changes to load images instead of embeddings.\n",
    "Note that we also apply a [random image augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/). \n",
    "\n",
    "```python\n",
    "            img = img_to_array(load_img(image, target_size=self.dim))\n",
    "            trans_args = self.img_gen.get_random_transform(self.dim)\n",
    "            img = self.img_gen.apply_transform(img, trans_args)\n",
    "            img = preprocess_input(img)\n",
    "            output_index = 2 * i\n",
    "\n",
    "            # Store similar sample\n",
    "            similar_img = self.__get_similar_image(image)\n",
    "            sim_img = img_to_array(load_img(similar_img, target_size=self.dim))\n",
    "            trans_args = self.img_gen.get_random_transform(self.dim)\n",
    "            sim_img = self.img_gen.apply_transform(sim_img, trans_args)\n",
    "            sim_img = preprocess_input(sim_img)\n",
    "            x[0][output_index,] = img\n",
    "            x[1][output_index,] = sim_img\n",
    "            y[output_index] = 1\n",
    "\n",
    "            # Store different sample\n",
    "            different_img = self.__get_different_image(image)\n",
    "            diff_img = img_to_array(\n",
    "                load_img(different_img, target_size=self.dim)\n",
    "            )\n",
    "            trans_args = self.img_gen.get_random_transform(self.dim)\n",
    "            diff_img = self.img_gen.apply_transform(diff_img, trans_args)\n",
    "            diff_img = preprocess_input(diff_img)\n",
    "            x[0][output_index + 1,] = img\n",
    "            x[1][output_index + 1,] = diff_img\n",
    "            y[output_index + 1] = 0\n",
    "```\n",
    "\n",
    "*Note: Make sure your downloaded the latest version of the code from our [repositiory](http://de-mucingode1.corp.capgemini.com/gitlab/SophieY/global_data_science_challenge_3_public)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_from_scratch.model'></a>\n",
    "### Updating the Model\n",
    "\n",
    "In a similar way we are updating our model definition. Instead of giving the embeddings as an input, we will train a proper siamese twin network with a MobileNet architecture for the CNN part:\n",
    "\n",
    "![siamese](https://miro.medium.com/max/1531/1*dFY5gx-Vze3micJ0AMVp0A.jpeg)\n",
    "\n",
    "To do this, we need to update our model definition function *siamese_net_from_embeddings* from:\n",
    "\n",
    "```python\n",
    "        input_shape = [EMBED_LENGTH]\n",
    "        embeddings_1 = Input(input_shape)\n",
    "        embeddings_2 = Input(input_shape)\n",
    "```\n",
    "\n",
    "to:\n",
    "\n",
    "```python\n",
    "        input_shape = [IMG_HEIGHT, IMG_WIDTH, 3]\n",
    "        left_input = Input(input_shape)\n",
    "        right_input = Input(input_shape)\n",
    "\n",
    "        model = MobileNet(input_shape=input_shape,\n",
    "                          weights='imagenet',\n",
    "                          include_top=False,\n",
    "                          pooling='avg')\n",
    "        \n",
    "        encoded_l = model(left_input)\n",
    "        encoded_r = model(right_input)\n",
    "```\n",
    "\n",
    "The resulting model is called *siamese_net_from_images_mobilenet* and can be found in the `src/model/siamese_twin_images.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on your local machine is possible, but will take a very very very long time. Which can be a good thing:\n",
    "![Training](xkcd_training.png)\n",
    "\n",
    "To train the model, the [CPU](https://en.wikipedia.org/wiki/Central_processing_unit) of our machine has to do a lot of matrix multiplications, a task it is not particularly good at. Fortunately, [GPU's](https://en.wikipedia.org/wiki/Graphics_processing_unit) are [much faster](https://graphics.stanford.edu/papers/gpumatrixmult/gpumatrixmult.pdf) doing these calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws'></a>\n",
    "# Speedings things up with GPUs: AWS SageMaker\n",
    "\n",
    "With AWS, we can rent GPUs and use them the speed up our training and predictions. AWS offers several tools to easily access their services. For the GDSC, we will be using [AWS SageMaker](https://aws.amazon.com/de/sagemaker/).\n",
    "\n",
    "> Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.\n",
    ">-- <cite>https://aws.amazon.com/sagemaker/</cite>\n",
    "\n",
    "To awesome thing about SageMaker is that is behaves *almost* as standard tensorflow, allowing us to easily switch from local to cloud development.\n",
    "\n",
    "### How does it work?\n",
    "What happens behind the scenes is described on a high level in the [docs](https://sagemaker.readthedocs.io/en/stable/using_tf.html#what-happens-when-fit-is-called) of the TensorFlow `.fit()` method that is part of the [AWS SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/).\n",
    "\n",
    "> Calling `fit` starts a SageMaker training job. The training job will execute the following:\n",
    ">\n",
    ">Starts `train_instance_count` EC2 instances of the type `train_instance_type`.\n",
    ">On each instance, it will do the following steps:\n",
    ">* starts a Docker container optimized for TensorFlow.\n",
    ">* downloads the dataset.\n",
    ">* setup up training related environment varialbes\n",
    ">* setup up distributed training environment if configured to use parameter server\n",
    ">* starts asynchronous training\n",
    ">-- <cite>https://sagemaker.readthedocs.io/</cite>\n",
    "\n",
    "This is quite a lot of things that are taken care of and we do not need to worry about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup'></a>\n",
    "# Setting up AWS SageMaker\n",
    "To set up AWS Sagemaker, we need to \n",
    "\n",
    "1. an AWS account setup and credentials for the same\n",
    "1. our local environment configured to be able to connect to it\n",
    "1. a locally running python application for the training logic __within \\*.py files__\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.credentials'></a>\n",
    "## AWS Account Credentials\n",
    "You can find your credentials in the profile tab of your [GDSC account](http://gdsc.ce.capgemini.com/aws_info/):\n",
    "\n",
    "* `Access Key ID` and `Secret Access Key` are the credentials to login to AWS\n",
    "* The `Team Name` is needed to start training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.account_setup'></a>\n",
    "## AWS Account Setup\n",
    "To use the AWS account, we need the `awscli` and the `sagemaker` package. Both can be installed with *pip*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (1.50.1)\n",
      "Requirement already satisfied: awscli in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (1.18.9)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (1.16.4)\n",
      "Requirement already satisfied: boto3>=1.10.32 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (1.12.9)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (1.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (2.22.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.2 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (0.1.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from sagemaker) (3.11.2)\n",
      "Requirement already satisfied: botocore==1.15.9 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (1.15.9)\n",
      "Requirement already satisfied: PyYAML<5.3,>=3.10 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (5.1.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (0.3.3)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (3.4.2)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (0.4.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from boto3>=1.10.32->sagemaker) (0.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from requests<3,>=2.20.0->sagemaker) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from requests<3,>=2.20.0->sagemaker) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from requests<3,>=2.20.0->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from requests<3,>=2.20.0->sagemaker) (1.25.7)\n",
      "Requirement already satisfied: six in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.13.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from protobuf>=3.1->sagemaker) (44.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from botocore==1.15.9->awscli) (2.8.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\dkuehlwe\\pycharmprojects\\global_data_science_challenge_3\\venv\\lib\\site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run \n",
    "\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "in the terminal to store our credentials. Enter the credentials from the previous step and confirm with *Enter*\n",
    "\n",
    "*Note:You can start the terminal via the Anaconda Launcher by selecting **CMD.exe Promt** in the applications tab.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to edit line 18 in the file `src/utils/remote_sagemaker.py`:\n",
    "\n",
    "```python\n",
    "TEAM_NAME = 'YOUR_TEAM_NAME'\n",
    "```\n",
    "\n",
    "Replace 'YOUR_TEAM_NAME' with the name of your team as shown in the profile tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.training_script'></a>\n",
    "## The Training Script\n",
    "\n",
    "So far, we relied on Jupyter notebooks to run our code. To use SageMaker, we need put everything in a Python script.\n",
    "The script will be uploaded to AWS and executed in the cloud. We prepared a basic version for this tutorial. You can find it under `src/local_training_siamese_mobilenet_from_images.py`. The main components are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts needs to be able to accept parameters from the command line. The [`__main__`](https://stackoverflow.com/questions/419163/what-does-if-name-main-do) check together with [argparse](https://docs.python.org/3/library/argparse.html) take care of this.\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=100)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--steps_per_epoch', type=int, default=100)\n",
    "    parser.add_argument('--validation_steps', type=int, default=10)\n",
    "\n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code follows our previous setup:\n",
    "\n",
    "```python\n",
    "    logging.info('Instantiating TrainGeneratorFromImages with args: %s' % args.train)\n",
    "    train_data_generator = BalancedDataGeneratorFromImages(args.train)\n",
    "    logging.info('Invoking siamese_net_from_images_mobilenet with learning_rate: %s' % args.learning_rate)\n",
    "    model = siamese_net_from_images_mobilenet(args.learning_rate)\n",
    "```\n",
    "\n",
    "creates the data generator and instanciates the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gets fitted on the data:\n",
    "\n",
    "```python\n",
    "    model.fit_generator(\n",
    "        train_data_generator,\n",
    "        steps_per_epoch=args.steps_per_epoch,\n",
    "        epochs=args.epochs,\n",
    "        callbacks=[tensorboard_callback, checkpoint_callback],\n",
    "    )\n",
    "```\n",
    "\n",
    "Note the two callbacks. [Tensorboard](https://www.tensorflow.org/tensorboard) allows you visualize the progress of your training, checkpoints store intermediate models so that you can go back to an earlier version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the predictions get created and written at\n",
    "\n",
    "\n",
    "```python\n",
    "    predictions = predict_siamese_twin_mobilenet_model(args.train, args.eval, model)\n",
    "    write_prediction_file(predictions, os.path.join(model_dir, \"predictions.csv\"))    \n",
    "```\n",
    "\n",
    "If we'd use the model to create the prediction it would take a **lot of time** because we'd have to compute the embeddings for every single pair. `predict_siamese_twin_mobilenet_model` first extracts the CNN of our model, precomputes all embeddings, and then only runs the embeddings comparison on all the pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to test that we made no mistakes by running the script on our local machine. To do this, open your terminal, go to your *src* folder and run:\n",
    "\n",
    "```bash\n",
    "python local_training_siamese_mobilenet_from_images.py --epochs=2 --steps_per_epoch=2 --learning-rate=0.0001 --batch_size=32 --train=../data/train --eval=../data/test_val\n",
    "```\n",
    "\n",
    "Note the small values for *epochs* and *steps_per_epoch*. We only want to test that the general setup works, not run a proper training. If everything worked, you'll the predictions under *src/trained_models/predicitions.csv*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- Test your script on your local machine before running it on SageMaker. This saves you time and money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.training_in_the_cloud'></a>\n",
    "## Training in the Cloud\n",
    "\n",
    "When you are sure that the training script runs properly on your local machine it is time for the next step: using your script to start a SageMaker training job. An example is provided under `src/remote_training_siamese_mobilenet_from_images.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script doesn't run SageMaker directly, but instead uses a custom backend since we share one AWS account among all participants. You can easily adapt it to your needs. Let's go over the main points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 08:07:37,765 remote_sagemaker retrieve_team_ddn_config_record line 53 Downloaded team config: {'team_name': 'Yeaaaaah', 'team_user_name': 'Yeaaaaah', 'team_role_name': 'Yeaaaaah-role', 'team_sm_role_name': 'arn:aws:iam::880110969874:role/smScalingQueuer-StartJobFunctionRole-1XX6KGVEV83UL', 'team_id': '8', 'team_region': 'us-west-2', 'team_regional_bucket_name': 'all-data-all-participants-us-west-2'} from URL: https://orfgr6uv4m.execute-api.eu-west-1.amazonaws.com/Prod/get_team_details/?team_name=Yeaaaaah\n",
      "2020-03-06 08:07:45,236 credentials load line 1196 Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2020-03-06 08:07:54,237 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\\.gitkeep to key Yeaaaaah/training_code_latest/data/.gitkeep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folder: C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 08:07:55,768 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\\embedding_generators.py to key Yeaaaaah/training_code_latest/data/embedding_generators.py\n",
      "2020-03-06 08:07:56,546 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\\image_generators.py to key Yeaaaaah/training_code_latest/data/image_generators.py\n",
      "2020-03-06 08:07:57,118 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\\make_dataset.py to key Yeaaaaah/training_code_latest/data/make_dataset.py\n",
      "2020-03-06 08:07:57,988 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\data\\__init__.py to key Yeaaaaah/training_code_latest/data/__init__.py\n",
      "2020-03-06 08:07:58,318 remote_sagemaker upload_code_folder_to_s3 line 177 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\local_training_siamese_mobilenet_from_images.py to key Yeaaaaah/training_code_latest/local_training_siamese_mobilenet_from_images.py\n",
      "2020-03-06 08:07:59,086 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\models\\siamese_twin_embeddings.py to key Yeaaaaah/training_code_latest/models/siamese_twin_embeddings.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folder: C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 08:07:59,614 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\models\\siamese_twin_images.py to key Yeaaaaah/training_code_latest/models/siamese_twin_images.py\n",
      "2020-03-06 08:08:00,153 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\models\\siamese_twin_predictions.py to key Yeaaaaah/training_code_latest/models/siamese_twin_predictions.py\n",
      "2020-03-06 08:08:00,628 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\models\\__init__.py to key Yeaaaaah/training_code_latest/models/__init__.py\n",
      "2020-03-06 08:08:01,223 remote_sagemaker upload_code_folder_to_s3 line 177 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\remote_training_siamese_mobilenet_from_images.py to key Yeaaaaah/training_code_latest/remote_training_siamese_mobilenet_from_images.py\n",
      "2020-03-06 08:08:01,774 remote_sagemaker upload_code_folder_to_s3 line 177 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\score.py to key Yeaaaaah/training_code_latest/score.py\n",
      "2020-03-06 08:08:02,306 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\utils\\remote_sagemaker.py to key Yeaaaaah/training_code_latest/utils/remote_sagemaker.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folder: C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 08:08:02,779 remote_sagemaker upload_code_folder_to_s3 line 160 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\utils\\__init__.py to key Yeaaaaah/training_code_latest/utils/__init__.py\n",
      "2020-03-06 08:08:03,326 remote_sagemaker upload_code_folder_to_s3 line 177 Uploading local file C:\\Users\\dkuehlwe\\PycharmProjects\\global_data_science_challenge_3_public\\src\\utils\\..\\__init__.py to key Yeaaaaah/training_code_latest/__init__.py\n",
      "2020-03-06 08:08:03,835 remote_sagemaker upload_code_folder_to_s3 line 184 Uploaded code to bucket all-data-all-participants-us-west-2 with prefix Yeaaaaah/training_code_latest\n"
     ]
    }
   ],
   "source": [
    "upload_code_folder_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = start_remote_sagemaker_job(\n",
    "    base_job_name='Test',\n",
    "    # This MUST point to a file, relative to the src/ folder.\n",
    "    # In this very example we use the provided local training script.\n",
    "    entry_point='local_training_siamese_mobilenet_from_images.py',\n",
    "    # Tweak your hyperparams here.\n",
    "    hyperparams={\n",
    "        'epochs': 1,\n",
    "        'learning_rate': 0.001, \n",
    "        'batch_size': 32,\n",
    "        'steps_per_epoch': 10\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the *entry_point* and *hyperparams* parameters. You might want to change them when you start creating your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test-Yeaaaaah-2020-03-06-07-35-06-033']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous script, you can start the script on the command line via\n",
    "\n",
    "```bash\n",
    "python remote_training_siamese_mobilenet_from_images.py \n",
    "```\n",
    "\n",
    "Note that you do not need to give any additional parameters since we hardcoded the values for epochs, learning_rate, etc in the script. You also do not neet to provide the *train* and *test_val* folders since they are already uploaded and referred for you.\n",
    "\n",
    "**You can only run one training job at once. You cannot start another job if your last one isn't finished.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.getting_the_results'></a>\n",
    "## Getting the results\n",
    "\n",
    "You can check the status of your training via the `get_job_details` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobName': 'Test-Yeaaaaah-2020-03-06-07-35-06-033',\n",
       " 'TrainingJobArn': 'arn:aws:sagemaker:us-west-2:880110969874:training-job/test-yeaaaaah-2020-03-06-07-35-06-033',\n",
       " 'ModelArtifacts': {'S3ModelArtifacts': 's3://all-data-all-participants-us-west-2/Yeaaaaah/trained_model_latest/Test-Yeaaaaah-2020-03-06-07-35-06-033/output/model.tar.gz'},\n",
       " 'TrainingJobStatus': 'Completed',\n",
       " 'SecondaryStatus': 'Completed',\n",
       " 'HyperParameters': {'batch_size': '32',\n",
       "  'epochs': '1',\n",
       "  'learning_rate': '0.001',\n",
       "  'model_dir': '\"s3://all-data-all-participants-us-west-2/Yeaaaaah/trained_model_latest/Test-Yeaaaaah-2020-03-06-07-35-06-033/model\"',\n",
       "  'sagemaker_container_log_level': '20',\n",
       "  'sagemaker_enable_cloudwatch_metrics': 'false',\n",
       "  'sagemaker_job_name': '\"Test-Yeaaaaah-2020-03-06-07-35-06-033\"',\n",
       "  'sagemaker_program': '\"local_training_siamese_mobilenet_from_images.py\"',\n",
       "  'sagemaker_region': '\"us-west-2\"',\n",
       "  'sagemaker_submit_directory': '\"s3://all-data-all-participants-us-west-2/Test-Yeaaaaah-2020-03-06-07-35-06-033/source/sourcedir.tar.gz\"',\n",
       "  'steps_per_epoch': '10'},\n",
       " 'AlgorithmSpecification': {'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.0.0-gpu-py3',\n",
       "  'TrainingInputMode': 'File',\n",
       "  'EnableSageMakerMetricsTimeSeries': True},\n",
       " 'RoleArn': 'arn:aws:iam::880110969874:role/smScalingQueuer-StartJobFunctionRole-1XX6KGVEV83UL',\n",
       " 'InputDataConfig': [{'ChannelName': 'train',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://all-data-all-participants-us-west-2/data/train/',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'},\n",
       "  {'ChannelName': 'eval',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://all-data-all-participants-us-west-2/data/test_val/',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'}],\n",
       " 'OutputDataConfig': {'KmsKeyId': '',\n",
       "  'S3OutputPath': 's3://all-data-all-participants-us-west-2/Yeaaaaah/trained_model_latest/'},\n",
       " 'ResourceConfig': {'InstanceType': 'ml.p2.xlarge',\n",
       "  'InstanceCount': 1,\n",
       "  'VolumeSizeInGB': 30},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 7200},\n",
       " 'CreationTime': '2020-03-06 07:35:08.405000+00:00',\n",
       " 'TrainingStartTime': '2020-03-06 07:37:35.036000+00:00',\n",
       " 'TrainingEndTime': '2020-03-06 07:47:57.125000+00:00',\n",
       " 'LastModifiedTime': '2020-03-06 07:47:57.125000+00:00',\n",
       " 'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "   'StartTime': '2020-03-06 07:35:08.405000+00:00',\n",
       "   'EndTime': '2020-03-06 07:37:35.036000+00:00',\n",
       "   'StatusMessage': 'Preparing the instances for training'},\n",
       "  {'Status': 'Downloading',\n",
       "   'StartTime': '2020-03-06 07:37:35.036000+00:00',\n",
       "   'EndTime': '2020-03-06 07:39:09.431000+00:00',\n",
       "   'StatusMessage': 'Downloading input data'},\n",
       "  {'Status': 'Training',\n",
       "   'StartTime': '2020-03-06 07:39:09.431000+00:00',\n",
       "   'EndTime': '2020-03-06 07:47:39.822000+00:00',\n",
       "   'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "  {'Status': 'Uploading',\n",
       "   'StartTime': '2020-03-06 07:47:39.822000+00:00',\n",
       "   'EndTime': '2020-03-06 07:47:57.125000+00:00',\n",
       "   'StatusMessage': 'Uploading generated training model'},\n",
       "  {'Status': 'Completed',\n",
       "   'StartTime': '2020-03-06 07:47:57.125000+00:00',\n",
       "   'EndTime': '2020-03-06 07:47:57.125000+00:00',\n",
       "   'StatusMessage': 'Training job completed'}],\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'EnableInterContainerTrafficEncryption': False,\n",
       " 'EnableManagedSpotTraining': False,\n",
       " 'TrainingTimeInSeconds': 622,\n",
       " 'BillableTimeInSeconds': 622,\n",
       " 'DebugHookConfig': {'S3OutputPath': 's3://all-data-all-participants-us-west-2/Yeaaaaah/trained_model_latest/',\n",
       "  'CollectionConfigurations': []},\n",
       " 'ResponseMetadata': {'RequestId': '58a67eba-1f0b-46ae-8c3e-8b2295571b92',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '58a67eba-1f0b-46ae-8c3e-8b2295571b92',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3421',\n",
       "   'date': 'Fri, 06 Mar 2020 07:49:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_details = get_job_details(job_name)\n",
    "job_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is finished you can download all created artifact, i.e. model checkpoint, logs and most importantly the created predictions with the S3 downloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 08:50:46,485 credentials load line 1196 Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "local_results_path = os.path.join('..', 'results')\n",
    "S3Downloader.download(job_details['ModelArtifacts']['S3ModelArtifacts'], local_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will put a file called *model.tar.gz* in the *results* folder. You can extract it with *7-zip*. Inside you'll find the your predictions in the file `predictions.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aws_setup.rules'></a>\n",
    "## Rules\n",
    "\n",
    "**Please be mindful when using AWS resources. We got a fixed budget that is shared among all participants.**\n",
    "\n",
    "When using AWS SageMaker we ask you to follow the following rules\n",
    "\n",
    "- Only use SageMaker when you really need GPU power.\n",
    "- Only use SageMaker for experiments that are well thought out and planned. You should have a clear goal and new learning for each run.\n",
    "- Only use SageMaker for the GDSC\n",
    "\n",
    "We keep track of how many resources each team uses. If you overspend, we may disable your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next_steps'></a>\n",
    "# Next steps\n",
    "\n",
    "- Do your research. How did other people solve similar problems? \n",
    "- Keep a list of things that could be improved. Prioritize them and do one experiment at a time. \n",
    "- *Optionally:* Read the trainings from the [last challenge](http://de-mucingode1.corp.capgemini.com/gitlab/dkuehlwein/global_data_science_challenge_2_public) on how to structure your experiments\n",
    "\n",
    "Some things we think can help you get a good result are:\n",
    "- Read the approaches of the [humpback detection competition](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563) on kaggle. There are many great ideas there. **The GDSC Tutorial submission with a score of 816 was created by finetuning this model.**\n",
    "- The triplet loss idea behind [FaceNet](https://arxiv.org/abs/1503.03832)\n",
    "- Experiment with different image sizes. Is 224x224 big enough to detect everything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "# Summary\n",
    "\n",
    "In this tutorial we\n",
    "- Analysed the problems of the embeddings model\n",
    "- Learned how to use AWS to train larger and better models\n",
    "- Discuss potential further improvements\n",
    "\n",
    "You can now start building your own models! Collaborate with the other participants on the low hanging fruits. \n",
    "Things like \n",
    "- code for cropping the fluke\n",
    "- finding data errors \n",
    "- setting up TensorBoard\n",
    "\n",
    "are best done in a team effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
