{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Building a first model\n",
    "\n",
    "In tutorial 1, we got a basic understanding of our problem. We learned that the images show over 1000 different whales. For the majority of them exists only a single image. \n",
    "We also considered the score function that determins the placings on the leaderboard. Recall that our task is:\n",
    "\n",
    "**For every image *i* in the *test_val* folder, predict the 20 images that are most similar to *i*.**\n",
    "\n",
    "In this tutorial, we will build two solutions to this problem. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "[1. Similarity between Images](#section1) <br>\n",
    "[2. A bit of history: The ImageNet Challenge](#section2) <br>\n",
    "&emsp; [2.1 What is a Deep Learning Model for image classification](#section2.1) <br>\n",
    "[3. On the shoulders of giants: Using pretrained model to convert images](#section3) <br>\n",
    "[4. How to measure similarity](#section4) <br>\n",
    "[5. Our baseline](#section5) <br>\n",
    "[6. Learn, don't define: A better similarity measure](#section6) <br>\n",
    "&emsp; [6.1 Examples for the AI: Creating image pairs ](#section6.1) <br>\n",
    "&emsp; [6.2 Feeding examples to the AI ](#section6.2) <br>\n",
    "&emsp; [6.3 Defining the model ](#section6.3) <br>\n",
    "&emsp; [6.4 Training the model ](#section6.4) <br>\n",
    "&emsp; [6.5 Creating predictions  ](#section6.5) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.src_data_process_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e13a6cdd393e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#from src.data.process_dataset import DataGeneratorFromEmbeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_data_process_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataGeneratorFromEmbeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src.src_data_process_dataset'"
     ]
    }
   ],
   "source": [
    "# For readability, we list all libraries we use in this notebook at the beginning\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from matplotlib.pyplot import imshow\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "#from src.data.process_dataset import DataGeneratorFromEmbeddings\n",
    "from src.src_data_process_dataset import DataGeneratorFromEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Similarity between images\n",
    "\n",
    "What makes two images similar? The colors? The shapes that appear? If you were to write an algorithm for it, how would you even start?\n",
    "\n",
    "Let's start by considering how a machine sees a picture. For a computer, an image is essentially a 3-d matrix. Take for example one of our whale fluke images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## On the shoulders of giants: Using pretrained model to convert images\n",
    "\n",
    "As described in the previous section, pre-trained deep learning models allow us to convert an image to a vector of size 1024 that contains the relevant information in the image. In this section, you'll learn how to do this.\n",
    "\n",
    "We'll use a model called [MobileNet](https://arxiv.org/pdf/1704.04861.pdf) that was trained on the ImageNet dataset for a first test. MobileNet is a second generation model that trades accuracy for speed. I.e. the vector representation will not be the best, but it will run on our laptops.\n",
    "In the next tutorial we will show you how to use AWS to take advantage of [more advanced models](https://keras.io/applications/) and even train your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "TRAIN_PATH = '../experiments/12. contour_edge_Li_tilt_noscaling_hsv-filter/train/'\n",
    "TEST_PATH = '../experiments/12. contour_edge_Li_tilt_noscaling_hsv-filter/test-val/'\n",
    "OUT_FILE = '../experiments/12. contour_edge_Li_tilt_noscaling_hsv-filter/' + 'siamese_li_edge_tilt_hsv_noscale_hsv-filter.csv'\n",
    "\n",
    "#TRAIN_PATH = '../data/train/'\n",
    "#TEST_PATH = '../data/test_val/'\n",
    "#OUT_FILE = './siamese_distance_only_preds_no-1.csv'\n",
    "\n",
    "model = MobileNet(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options *include_top=False* and *pooling='avg'* are necessary to extract the *logits* layer. \n",
    "The *input_shape* defines the size of the images. The default for ImageNet models is 224x224 pixels with 3 channels for colors.\n",
    "We can check out the architecture of the model with the *summary* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Our baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are aware of the concept, we are one step closer to our first submission. We will \n",
    "\n",
    "- use Mobilenet to compute the embeddings for all of the train and test pictures \n",
    "- calculate the cosine similarity between each of the embeddings of the test pictures and the embeddings of the train pictures\n",
    "- create a submission that predicts the 20 matches with the highest similarity score\n",
    "\n",
    "We start with loading all pictures with [Tensorflow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).As first step we are going to list all file paths and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = Path(TRAIN_PATH)\n",
    "train_files_paths = list(train_data_path.glob('*/*.jpg'))  # All file paths\n",
    "\n",
    "###### MY IMPLEMENTATION: Remove class -1\n",
    "train_files_paths_filtered = []\n",
    "for file_path in train_files_paths:\n",
    "    if not('\\\\-1\\\\' in str(file_path)):\n",
    "        train_files_paths_filtered.append(file_path)\n",
    "train_files_paths = train_files_paths_filtered\n",
    "###### Back to Tutorial 2 #########\n",
    "\n",
    "train_files_names = [p.name for p in train_files_paths]  # Only the file names without the full path\n",
    "train_files_classes = [p.parent.name for p in train_files_paths]  # The parent directory, i.e. the whale ID, for each image\n",
    "\n",
    "test_data_path = Path(TEST_PATH)\n",
    "test_files_paths = list(test_data_path.glob('*.jpg'))\n",
    "test_files_names = [p.name for p in test_files_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3956"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a single picture we have to read the file, convert it into a [tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) object, adjust the size and standardize it to the MobileNet input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])  # Resize image to 224x224 pixels\n",
    "    img = preprocess_input(img)  # Do additional preprocessing that is required for MobileNet\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the prerequisites to load a single picture, we can apply it to all of our pictures. We create a dataset from our input data paths and apply the defined function to each entry in the dataset.\n",
    "\n",
    "The batch size specifies the number of examples we use in one iteration of calcuation. For now you can ignore it. It will become more relevant later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_files = tf.data.Dataset.from_tensor_slices(list(map(str, train_files_paths)))  # Convert paths to strings\n",
    "train_ds = train_files.map(load_image)  # Apply the load_image function to every image\n",
    "train_ds = train_ds.batch(BATCH_SIZE)  # Set the batch size\n",
    "\n",
    "test_files = tf.data.Dataset.from_tensor_slices(list(map(str, test_files_paths)))\n",
    "test_ds = test_files.map(load_image)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the embeddings we use the Mobilenet. This will take a while, so time to grab a coffe while this is running :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed_train = model.predict(train_ds)\n",
    "embed_test = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training embeddings (3956, 1024)\n",
      "Shape of test embeddings (808, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of training embeddings {embed_train.shape}')\n",
    "print(f'Shape of test embeddings {embed_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every picture we have now the representation as a vector with a size of 1024. We can calculate the cosine similarity between all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all embeddings (4764, 1024)\n"
     ]
    }
   ],
   "source": [
    "all_names = np.concatenate((train_files_names, test_files_names))  # Combine the train and test data\n",
    "all_paths = np.concatenate((train_files_paths, test_files_paths))\n",
    "embed_all = np.concatenate((embed_train, embed_test))\n",
    "print(f'Shape of all embeddings {embed_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the similarity matrix (808, 4764)\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(embed_test, embed_all)\n",
    "print(f'Shape of the similarity matrix {similarity_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 808 pictures in the test dataset we have the similarity to each of the 5340 other picutres. Let us take a look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity of picture 666 and 2656 is 0.7585594654083252\n"
     ]
    }
   ],
   "source": [
    "index_1 = 666\n",
    "index_2 = 2656\n",
    "print(f'The cosine similarity of picture {index_1} and {index_2} is {similarity_matrix[index_1, index_2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also at the two respective pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(img_paths):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    image_count = len(img_paths)\n",
    "    for path, n in zip(img_paths, range(image_count)):\n",
    "        ax = plt.subplot(image_count, 4, n + 1)\n",
    "        img = load_img(path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{path.name}, class: {path.parent.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images([all_paths[index_1], all_paths[index_2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a submission, we sort the similarities in descending order and write the twenty most similar pictures in to a file. We use the [np.argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) function, which will return the indices to sort our matrix column-wise in ascending order. Since we want to have the direction descending, we will put a **-** before the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(-similarity_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the format for a submissions is\n",
    "\n",
    "*test_pic, predicted_pic_1, predicted_pic_2, ...., predicted_pic_20*\n",
    "\n",
    "The cosine similarity between a picture and itself is always 1.0. Hence the most similar image is always the image itself and we can create a submission as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('cosine_preds.csv', 'w') as f:\n",
    "#    for i in range(len(test_files_names)):\n",
    "#        indices_of_top_predictions = indices[i,:21]  # Get the indices of the 21 most similar images\n",
    "#        names_of_top_predictions = all_names[indices_of_top_predictions]  # Get the file names of the 21 most similar images\n",
    "#        preds = ','.join(names_of_top_predictions)  # Put a , between the image names\n",
    "#        f.write(preds + '\\n')  # Write it in a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we upload the submission we get a score of *83.00*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**Best practice:** \n",
    "- Deep Learning models are state-of-the-art for almost every image-type problem\n",
    "- You can use pretrained models to compress the data in an image\n",
    "- The cosine is an good baseline for similarity between vectors\n",
    "- When tackling an AI problem, always start with a simple model. This is called the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "# Learn, don't define: A better similarity measure\n",
    "\n",
    "The baseline model took existing concepts and applied them to our problem. In other words, we took a standard software-engineering approach. The drawback of this approach is that that we have to manually define every single step. \n",
    "\n",
    "Now let us solve the problem with artificial intelligence, or rather machine learning. Here's the battle plan:\n",
    "\n",
    "- Goal: Our model should learn whether or not two images show the same whale.\n",
    "- Data: Create a dataset of images pairs and a label that denotes if the two images show the same whale.\n",
    "- Architecture: A [Siamese Twin Network](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d) fits perfectly to our use case\n",
    "\n",
    "![siamese](https://miro.medium.com/max/1531/1*dFY5gx-Vze3micJ0AMVp0A.jpeg)\n",
    "\n",
    "As the image illustrates, a Siamese Twin Network \n",
    "\n",
    "- takes two images as input\n",
    "- embeds them via a convolutional neural network\n",
    "- computes the absolute difference between the two embeddings\n",
    "- predicts whether or not the images are equal based on the difference.\n",
    "\n",
    "Note that our baseline model is very similar to the siamese twin approach. We computed embeddings with MobileNet and instead of learning the similarity from the absolute difference of the embeddings we used the cosine similarity.\n",
    "\n",
    "*For our next model, we'll keep the MobileNet embeddings and replace the cosine similarity with a learned distance function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- Create an architecture image to illustrate and discuss deep learning models\n",
    "- Learn the different types of layers and when to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.1'></a>\n",
    "## Examples for the AI: Creating image pairs \n",
    "\n",
    "For the input we need to create pairs of images with a label that is *1* if the images show the same whale and *0* if the images show a different whale. First we create pairs of images with the [combinations](https://stackoverflow.com/questions/942543/operation-on-every-pair-of-element-in-a-list) function.\n",
    "\n",
    "**Exercise:** Read up on how exactly the *combinations* works. Is this the best way to create image pairs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = list(combinations(zip(train_files_names, train_files_classes), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*train_pairs* consists of tuples where the first entry is the name of the file and the second the corresponding class. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('PM-WWA-20160724-126.jpg', '0161'), ('PM-WWA-20150426-061.jpg', '0836'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the information about the similarity. Two images are similar [iff](https://en.wikipedia.org/wiki/If_and_only_if) they have the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(pair):\n",
    "    image_one_class = pair[0][1]\n",
    "    image_two_class = pair[1][1]\n",
    "    if image_one_class == image_two_class:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = {pair: is_similar(pair) for pair in train_pairs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example pair from above, both images have the class *-1*. Hence the corresponding label is *1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7822990 pairs. 7044 matched pairs (0.09%); 7815946 unmatched pairs (99.91%)\n"
     ]
    }
   ],
   "source": [
    "# How many pairs are there?\n",
    "total_pairs = len(train_pairs)\n",
    "matched_pairs = sum(x == 1 for x in train_labels.values())\n",
    "unmatched_pairs = sum(x == 0 for x in train_labels.values())\n",
    "print(\"{} pairs. {} matched pairs ({}%); {} unmatched pairs ({}%)\".format(total_pairs,matched_pairs,round(matched_pairs/total_pairs,4)*100,unmatched_pairs,round(unmatched_pairs/total_pairs,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7044"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x == 1 for x in train_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3956"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.2'></a>\n",
    "## Feeding examples to the AI\n",
    "Now we have all the information we need and we can start loading the pictures. Since the size of our data might not fit into RAM all at once, we are using the [DataGenerator](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) class that Keras offers. The DataGenerator class continuously loads our data during the training process.\n",
    "In order to do modelling, we have to define on which data our model can train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = {img: embedding for img, embedding in zip(train_files_names, embed_train)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the computations, we create a dictionary, i.e. a lookup table, that given the name of a image file returns the precomputed embedding. Together with the pairs and labels we created in the previous section, we can create our own *DataGenerator*.\n",
    "The details will be covered in the next tutorial. If you can't wait, the source code lies in *src.data.process_dataset.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGeneratorFromEmbeddings(train_pairs, train_embeddings, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *DataGenerator* creates tupels of inputs (i.e. embeddings) and expected outputs (i.e. labels) that we can use to train our model. Let us take a look on one example. Per default, one sample consists of 32 embeddings and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded pictures are in shape (32, 1024)\n",
      "The loaded labels are in shape (32,)\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = train_generator.get_sample(0)\n",
    "print(f'The loaded pictures are in shape {embeddings[0].shape}')\n",
    "print(f'The loaded labels are in shape {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- Use a DataGenerator to feed data into your model. \n",
    "- Deep Learning uses a lot of resources. Precomputing and caching data can save a lot of time.\n",
    "- Split the data into a [train and validation subset](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) to evaluate the performance of your model during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Split the data into train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.3'></a>\n",
    "## Defining the model\n",
    "The next step in our process is the turn the image into an actual model. The following code defines a model that implements the siamese twin network shown above and compiles it.\n",
    "\n",
    "*Note: Usually, you do not need to compile Python code. Tensorflow models are an exception since they are focussed on performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_LENGTH = 1024\n",
    "\n",
    "def model_mobilenet_fn(learning_rate):    \n",
    "    \"\"\"\n",
    "    Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    input_shape = [EMBED_LENGTH]\n",
    "    embeddings_1 = Input(input_shape)\n",
    "    embeddings_2 = Input(input_shape)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings \n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    absolute_difference = L1_layer([embeddings_1, embeddings_2])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    similarity_score = Dense(1, activation='sigmoid', \n",
    "                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01, seed=None))(absolute_difference)\n",
    "    \n",
    "    siamese_net = Model(inputs=[embeddings_1,embeddings_2], outputs=similarity_score)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an actual instance of the model, we only need to specify the [learning rate](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10). The *summary* function gives an overview of our creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1024)         0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,025\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "#learning_rate = 0.00001\n",
    "model = model_mobilenet_fn(learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila, we just created our first deep learning model with a measly 1025 different parameters that need to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.4'></a>\n",
    "## Training the model\n",
    "To make the model learn, we pass image, label pairs to the model. We just have to define the number of times the model is trained with our pairs. This is done with the following parameters\n",
    "\n",
    "* *batch size* is the number of examples used in one training iteration. A higher number leads to more robust results. \n",
    "* *epochs* is the number of iterations\n",
    "* *steps_per_epoch* is the number of batches the model considers per epoch\n",
    "\n",
    "Recall that, we are giving the images as the input to our model and compare the output with the expected output (our label). To measure how good the model is in the current training period, we are using the [binary cross-entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.2826 - accuracy: 0.9444\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0767 - accuracy: 0.9981\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0288 - accuracy: 0.9984\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0137 - accuracy: 0.9997\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0107 - accuracy: 0.9997\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0149 - accuracy: 0.9984\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9978\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1923f20b288>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out the right now for *epochs*, *steps_per_epoch* and *batch_size* and *learning_rate* is a science in itself.\n",
    "In general, when there are only minor changes in loss and acc, either stop the training or decrease the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- When there are only minor changes in loss and acc, either stop the training or decrease the learning rate.\n",
    "- When you set up a separate validation, stop training once the loss and acc is worse on validation than on the training data set. In this case you started to [overfit](https://en.wikipedia.org/wiki/Overfitting)\n",
    "- Keep track of all the results you get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.5'></a>\n",
    "## Creating predictions \n",
    "Our model is trained and we can now use it to make predictions to the *test_val* dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {img: embedding for img, embedding in zip(all_names, embed_all)}\n",
    "all_file_count = len(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute the predictions for a single image, we need to pair it with every other image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = test_files_names[0]\n",
    "mask = all_names != test_file_name  \n",
    "all_other_images = all_names[mask]  # Filter out the test_file_name with the mask\n",
    "assert len(all_other_images) == (all_file_count - 1)  # Use asserts to quickly check assumptions  \n",
    "test_file_pairs = [(test_file_name, other_image) for other_image in all_other_images]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create a new DataGenerator to create the actual predictions. \n",
    "\n",
    "**Important**: To ensure, that we are predicting for each pair only one similarity, the batch size has to be a multiple of the number of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction_generator = DataGeneratorFromEmbeddings(test_file_pairs, all_embeddings, labels=None, batch_size=19, shuffle=False)\n",
    "predictions = model.predict_generator(prediction_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we use *argsort* to sort the results and extract the top 20 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(-predictions, axis=0)\n",
    "names_of_top_predictions = all_other_images[indices[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the individual steps working, we can now compute the predictions for every test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10/808 images (1.24%)\n",
      "processed 20/808 images (2.48%)\n",
      "processed 30/808 images (3.71%)\n",
      "processed 40/808 images (4.95%)\n",
      "processed 50/808 images (6.19%)\n",
      "processed 60/808 images (7.43%)\n",
      "processed 70/808 images (8.66%)\n",
      "processed 80/808 images (9.9%)\n",
      "processed 90/808 images (11.14%)\n",
      "processed 100/808 images (12.38%)\n",
      "processed 110/808 images (13.61%)\n",
      "processed 120/808 images (14.85%)\n",
      "processed 130/808 images (16.09%)\n",
      "processed 140/808 images (17.33%)\n",
      "processed 150/808 images (18.56%)\n",
      "processed 160/808 images (19.8%)\n",
      "processed 170/808 images (21.04%)\n",
      "processed 180/808 images (22.28%)\n",
      "processed 190/808 images (23.51%)\n",
      "processed 200/808 images (24.75%)\n",
      "processed 210/808 images (25.99%)\n",
      "processed 220/808 images (27.23%)\n",
      "processed 230/808 images (28.47%)\n",
      "processed 240/808 images (29.7%)\n",
      "processed 250/808 images (30.94%)\n",
      "processed 260/808 images (32.18%)\n",
      "processed 270/808 images (33.42%)\n",
      "processed 280/808 images (34.65%)\n",
      "processed 290/808 images (35.89%)\n",
      "processed 300/808 images (37.13%)\n",
      "processed 310/808 images (38.37%)\n",
      "processed 320/808 images (39.6%)\n",
      "processed 330/808 images (40.84%)\n",
      "processed 340/808 images (42.08%)\n",
      "processed 350/808 images (43.32%)\n",
      "processed 360/808 images (44.55%)\n",
      "processed 370/808 images (45.79%)\n",
      "processed 380/808 images (47.03%)\n",
      "processed 390/808 images (48.27%)\n",
      "processed 400/808 images (49.5%)\n",
      "processed 410/808 images (50.74%)\n",
      "processed 420/808 images (51.98%)\n",
      "processed 430/808 images (53.22%)\n",
      "processed 440/808 images (54.46%)\n",
      "processed 450/808 images (55.69%)\n",
      "processed 460/808 images (56.93%)\n",
      "processed 470/808 images (58.17%)\n",
      "processed 480/808 images (59.41%)\n",
      "processed 490/808 images (60.64%)\n",
      "processed 500/808 images (61.88%)\n",
      "processed 510/808 images (63.12%)\n",
      "processed 520/808 images (64.36%)\n",
      "processed 530/808 images (65.59%)\n",
      "processed 540/808 images (66.83%)\n",
      "processed 550/808 images (68.07%)\n",
      "processed 560/808 images (69.31%)\n",
      "processed 570/808 images (70.54%)\n",
      "processed 580/808 images (71.78%)\n",
      "processed 590/808 images (73.02%)\n",
      "processed 600/808 images (74.26%)\n",
      "processed 610/808 images (75.5%)\n",
      "processed 620/808 images (76.73%)\n",
      "processed 630/808 images (77.97%)\n",
      "processed 640/808 images (79.21%)\n",
      "processed 650/808 images (80.45%)\n",
      "processed 660/808 images (81.68%)\n",
      "processed 670/808 images (82.92%)\n",
      "processed 680/808 images (84.16%)\n",
      "processed 690/808 images (85.4%)\n",
      "processed 700/808 images (86.63%)\n",
      "processed 710/808 images (87.87%)\n",
      "processed 720/808 images (89.11%)\n",
      "processed 730/808 images (90.35%)\n",
      "processed 740/808 images (91.58%)\n",
      "processed 750/808 images (92.82%)\n",
      "processed 760/808 images (94.06%)\n",
      "processed 770/808 images (95.3%)\n",
      "processed 780/808 images (96.53%)\n",
      "processed 790/808 images (97.77%)\n",
      "processed 800/808 images (99.01%)\n"
     ]
    }
   ],
   "source": [
    "#%%time # usually it takes ~ 8 mins\n",
    "i=0\n",
    "img_count = len(test_files_names)\n",
    "\n",
    "siamese_distance_only_preds = []\n",
    "for test_file_name in test_files_names:\n",
    "    mask = all_names != test_file_name\n",
    "    all_other_images = all_names[mask]  \n",
    "    assert len(all_other_images) == (all_file_count - 1)   \n",
    "    test_file_pairs = [(test_file_name, other_image) for other_image in all_other_images]  \n",
    "    \n",
    "    prediction_generator = DataGeneratorFromEmbeddings(test_file_pairs, all_embeddings, labels=None, batch_size=19, shuffle=False)\n",
    "    predictions = model.predict_generator(prediction_generator)\n",
    "    \n",
    "    indices = np.argsort(-predictions, axis=0)\n",
    "    names_of_top_predictions = all_other_images[indices[:20].reshape(-1)]\n",
    "    siamese_distance_only_preds.append(np.concatenate(([test_file_name], names_of_top_predictions)))\n",
    "    \n",
    "    #progress tracking\n",
    "    i += 1\n",
    "    if (i%10 == 0):\n",
    "        print(\"processed {}/{} images ({}%)\".format(i,img_count,round(i/img_count*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a new submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUT_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-920aee77868d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUT_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msiamese_distance_only_preds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OUT_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "with open(OUT_FILE, 'w') as f:\n",
    "    for entry in siamese_distance_only_preds:\n",
    "        preds = ','.join(entry)  \n",
    "        f.write(preds + '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon uploading the file, we get a score of *16.01* which is **worse than our baseline**.\n",
    "What is going on?\n",
    "\n",
    "**Excercise:**\n",
    "- Figure out the mistakes we did when training the model and fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Best practice:** \n",
    "    \n",
    "- If your results are worse than your baseline, you probably made a very basic mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this tutorial we learned about deep learning and how it revolutionized image processing.\n",
    "We build a baseline model based on image embeddings and the cosine similarity and finally trained a first deep learning model.\n",
    "Unfortunately, the results we got were worse than our baseline.\n",
    "\n",
    "In the next tutorial we will\n",
    "- Analyse and fix the problems in our current model\n",
    "- Learn how to use AWS to train larger and better models\n",
    "- Discuss potential further improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
